{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlantDisPre \n",
    "[link for working prototype: https://plantdispre.herokuapp.com](https://plantdispre.herokuapp.com)\n",
    "## an application to predict the health  and disease of a plant\n",
    "## providing natural remidies to prevent those problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite: We will use the [PlantVillage dataset](https://github.com/spMohanty/PlantVillage-Dataset/tree/master/data_distribution_for_SVM) and it is a PyTorch implementation of [Using Deep Learning for Image-Based Plant Disease Detection](https://arxiv.org/ftp/arxiv/papers/1604/1604.03169.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEqZ0WVKhiX"
   },
   "source": [
    "We will be using Densenet121 and using transfer learning approach to train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installing PyTorch and loading the dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9i7KcUqERU81",
    "outputId": "2e0ed458-75ff-40d2-fafc-7d836f284466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "path = '/gdrive/My Drive/plant'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing torch and checking for availability of cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-8zBfvKlGB33",
    "outputId": "aee28c12-3417-43ee-dfa3-e4d7d9a82506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# Imports here\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qnn7QvCGB4F"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "Here we are using `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html))\n",
    "the data is divided into train and validation sets\n",
    "\n",
    "For training to attain generalisation and to overcomme overfitting we are goig to apply some transforms  such as random rotation, random resizing and croping, flipping etcetera.\n",
    "\n",
    "As validation set is used to measure the  performance of model on the data  it hasn't seen. So we only resize and crop to appropriate size.\n",
    "\n",
    "As The pre-trained networks available from `torchvision` were trained on the ImageNet dataset where each color channel was normalized separately. \n",
    "So For both sets we  need to normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKJGe9RYGB4X"
   },
   "outputs": [],
   "source": [
    "data_dir = path \n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "\n",
    "# TODO: Define your transforms for the training and validation sets\n",
    "#data_transforms = \n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224, scale=(0.7, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
    "                                       transforms.ColorJitter(.15,.15,.15,.15),\n",
    "       \n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomVerticalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                     \n",
    "                                       transforms.Normalize([.485,.456,.406],\n",
    "                                                            [.229,.224,.225])])\n",
    "valid_transforms = transforms.Compose([transforms.Resize(240),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([.485,.456,.406],\n",
    "                                                            [.229,.224,.225])])\n",
    "\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "#image_datasets = \n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "#dataloaders = \n",
    "batchSize = 32\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batchSize, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=batchSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeuazPqtGB4z"
   },
   "source": [
    "using Densenet121 and chaning the classifier part with two hidden layers. Here Number of outputs is 38.\n",
    "We wull be using Adam as optimizer and ReduceLROnPlateau as scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "MFEtK5gSwbz8",
    "outputId": "8bb6f82e-2016-4b2c-9020-b3beaed66e3f"
   },
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "model = models.densenet121(pretrained = True)\n",
    "from collections import OrderedDict\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                                ('fc1',nn.Linear(1024,512)),\n",
    "                                ('relu',nn.ReLU()),\n",
    "                                ('fc2',nn.Linear(512,38)),                            \n",
    "                                ('output',nn.LogSoftmax(dim=1))]))\n",
    "model.classifier = classifier\n",
    "criterion = nn. NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(),lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "#checking for GPU\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "LOYn7O5XHbmS",
    "outputId": "1cd8a505-b285-4186-faad-5a06d054e679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.309208 \tValidation Loss: 0.607066\tValidation Accuracy: 0.806011\n",
      "Validation loss decreased (inf --> 0.607066).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.533311 \tValidation Loss: 0.388867\tValidation Accuracy: 0.870959\n",
      "Validation loss decreased (0.607066 --> 0.388867).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.415532 \tValidation Loss: 0.321552\tValidation Accuracy: 0.892197\n",
      "Validation loss decreased (0.388867 --> 0.321552).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.367352 \tValidation Loss: 0.314290\tValidation Accuracy: 0.892576\n",
      "Validation loss decreased (0.321552 --> 0.314290).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.357933 \tValidation Loss: 0.278741\tValidation Accuracy: 0.904049\n",
      "Validation loss decreased (0.314290 --> 0.278741).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.316591 \tValidation Loss: 0.245641\tValidation Accuracy: 0.919503\n",
      "Validation loss decreased (0.278741 --> 0.245641).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.309144 \tValidation Loss: 0.264008\tValidation Accuracy: 0.910022\n",
      "Epoch: 8 \tTraining Loss: 0.273042 \tValidation Loss: 0.241049\tValidation Accuracy: 0.917607\n",
      "Validation loss decreased (0.245641 --> 0.241049).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.275159 \tValidation Loss: 0.248072\tValidation Accuracy: 0.915047\n",
      "Epoch: 10 \tTraining Loss: 0.274343 \tValidation Loss: 0.199535\tValidation Accuracy: 0.929933\n",
      "Validation loss decreased (0.241049 --> 0.199535).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.250964 \tValidation Loss: 0.297177\tValidation Accuracy: 0.905281\n",
      "Epoch: 12 \tTraining Loss: 0.251600 \tValidation Loss: 0.196101\tValidation Accuracy: 0.931165\n",
      "Validation loss decreased (0.199535 --> 0.196101).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.240409 \tValidation Loss: 0.225292\tValidation Accuracy: 0.922158\n",
      "Epoch: 14 \tTraining Loss: 0.237304 \tValidation Loss: 0.216928\tValidation Accuracy: 0.927183\n",
      "Epoch: 15 \tTraining Loss: 0.225954 \tValidation Loss: 0.209591\tValidation Accuracy: 0.930786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    model.train()\n",
    "    for data, target in trainloader:\n",
    "        #if GPU is available move them to GPU\n",
    "        if train_on_gpu:\n",
    "            data,target  = data.cuda(), target.cuda()\n",
    "        #clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Computing the outputs by psiing inputs to the model \n",
    "        output = model(data)\n",
    "        #calculating batch loss\n",
    "        loss = criterion(output, target)\n",
    "        #compute the gradients for backpropogation\n",
    "        loss.backward()\n",
    "        #updating the parameters by optimization step\n",
    "        optimizer.step()\n",
    "        #Update training loss\n",
    "        train_loss+= loss.item()*data.size(0)\n",
    "        \n",
    "    #validation phase\n",
    "    model.eval()\n",
    "    for data, target in validloader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss+= loss.item()*data.size(0)\n",
    "        #accuracy\n",
    "        ps = torch.exp(output)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        accuracy += torch.sum(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    accuracy = accuracy/len(validloader.dataset)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    #printing training and validation statistics\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\\tValidation Accuracy: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss, accuracy))\n",
    "    #saving the model if validation loss has decreased\n",
    "    if valid_loss<=valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'plantDensenet121.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0J58AGD7d_MP"
   },
   "source": [
    "# testing accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d89GmOZDeC81",
    "outputId": "7f97b8a7-84f8-4aa1-d012-3bfab548c693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, targets in testloader:\n",
    "        if train_on_gpu:\n",
    "            data, targets = data.cuda(), targets.cuda()\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+=targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        \n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "plant Densenet Image Classifier Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
